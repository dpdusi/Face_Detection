{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Face Recognition-Based Surveillance System**\n",
        "\n",
        "# **Project Overview:**\n",
        "\n",
        "The primary goal of this project is to develop a robust face recognition-based surveillance system that processes live video feeds from CCTV cameras. The system is designed to:\n",
        "\n",
        "*   Detect faces in real-time.\n",
        "*   Identify and recognize individuals.\n",
        "*   Record the names and entry timestamps of recognized individuals into a structured format (CSV/Excel).\n",
        "*   Ensure high accuracy, scalability, and efficient processing without dropping frames.\n",
        "\n",
        "# **Motivation and Objectives**\n",
        "\n",
        "With increasing demand for secure environments, face recognition systems play a pivotal role in automating surveillance tasks. This project aims to:\n",
        "\n",
        "\n",
        "*   Provide a reliable solution for real-time face detection and recognition.*\n",
        "*   Enhance the scalability of face recognition systems for large datasets.\n",
        "*   Compare performance metrics of advanced face recognition models (e.g., DeepFace and FaceNet).\n",
        "*   Establish a framework that integrates seamlessly with existing CCTV setups.\n",
        "\n",
        "# **Key Components**\n",
        "\n",
        "This project is divided into several key steps to ensure a systematic approach and effective implementation. Each step builds upon the previous one, leading to a cohesive and functional system. The steps are as follows:\n",
        "\n",
        "\n",
        "**1.   Acquire images from CCTV cameras:**\n",
        "\n",
        "Establish a pipeline to fetch live video feeds from CCTV cameras while ensuring no frames are dropped during processing.\n",
        "\n",
        "**2.   Develope a face detection model:**\n",
        "\n",
        "Train and deploy the YOLOv8 model to detect faces in the captured video frames. This step focuses on accurately localizing faces with minimal latency.\n",
        "\n",
        "**3.   Create Embeddings and recognise faces:**\n",
        "\n",
        "Use the FaceNet model to generate embeddings for the detected faces.\n",
        "Employ cosine similarity to compare embeddings with the database and identify individuals.\n",
        "\n"
      ],
      "metadata": {
        "id": "Tba2ZjVehZHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Acquire images from CCTV cameras**\n",
        "\n",
        "We extracted and saved frames from a live CCTV feed using the RTSP protocol. To access the feed, we constructed a stream URL using the provided IP address, port, username, and password. Frames were captured in batches, with each batch containing a specific number of frames, and the frames were saved as image files in a designated directory. This process was repeated for a set number of batches to ensure efficient and organized storage of frames for further use. We also incorporated error handling to address potential issues such as failed stream access or frame capture."
      ],
      "metadata": {
        "id": "YIRngUh-mrvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import urllib.parse\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Directory to save extracted frames\n",
        "save_dir = \"/content/drive/My Drive/LiveFeedFrames\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "def save_frames_in_batches(ip, port, username, password, camerapath, save_dir, batch_size=10, total_batches=5):\n",
        "    username = urllib.parse.quote(username)\n",
        "    password = urllib.parse.quote(password)\n",
        "\n",
        "    stream_url = f'rtsp://{username}:{password}@{ip}:{port}/{camerapath}'\n",
        "    cap = cv2.VideoCapture(stream_url)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Failed to open video stream.\")\n",
        "        return\n",
        "\n",
        "    batch_count = 0\n",
        "    while batch_count < total_batches:\n",
        "        print(f\"Saving batch {batch_count + 1}/{total_batches}\")\n",
        "        for i in range(batch_size):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                print(\"Failed to read frame.\")\n",
        "                break\n",
        "\n",
        "            # Save the frame as an image\n",
        "            frame_index = batch_count * batch_size + i\n",
        "            frame_path = os.path.join(save_dir, f\"frame_{frame_index:04d}.jpg\")\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            print(f\"Saved: {frame_path}\")\n",
        "\n",
        "        batch_count += 1\n",
        "        print(f\"Batch {batch_count} saved.\")\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Saved {batch_count * batch_size} frames in total.\")\n",
        "\n",
        "# Parameters for camera connection\n",
        "ip = \"202.53.85.100\"\n",
        "username=\"admin\"\n",
        "password = \"smart@123\"\n",
        "port=\"8888\"\n",
        "camerapath=\"live\"\n",
        "\n",
        "# Extract and save frames in batches\n",
        "save_frames_in_batches(ip, port, username, password, camerapath, save_dir, batch_size=10, total_batches=10)\n"
      ],
      "metadata": {
        "id": "4s8-Mg8ubESu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Develope a face detection model**\n",
        "\n",
        "We trained a custom YOLOv8 model for face detection using a labeled dataset. Since YOLOv8 is a general-purpose object detection model, and our requirement is to detect only faces, we retrained the model using a dataset specifically labeled for faces to create a tailored face detection model. The dataset structure was defined in a YAML configuration file, specifying the paths for training and validation images, the number of classes, and their names. We initialized the YOLOv8 model and trained it for 50 epochs with a batch size of 16 and an image size of 640 pixels, saving the weights and results under a specified name. After training, the model's performance was validated, and it was used to predict and save results for a sample image, ensuring precise face detection."
      ],
      "metadata": {
        "id": "X0wivo84bSJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics==8.0.20\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "dataset_yaml = \"\"\"\n",
        "path: /content/drive/MyDrive/Dataset            # Path to your dataset folder\n",
        "train: /content/drive/MyDrive/Dataset/train     # Relative path to training images\n",
        "val: /content/drive/MyDrive/Dataset/val         # Relative path to validation images\n",
        "\n",
        "# Number of classes and class names\n",
        "nc: 1\n",
        "names: ['face']\n",
        "\"\"\"\n",
        "with open('face_detection.yaml', 'w') as f:\n",
        "    f.write(dataset_yaml)\n",
        "\n",
        "# Load YOLOv8 model\n",
        "model = YOLO(\"yolov8n.pt\")  # Choose appropriate YOLO version\n",
        "\n",
        "# Train the model with custom callback\n",
        "model.train(\n",
        "    data=\"face_detection.yaml\",\n",
        "    epochs=50,\n",
        "    batch=16,\n",
        "    imgsz=640,\n",
        "    device=0,\n",
        "    name='yolov8_face_detection' # Name for saving weights and results\n",
        ")\n",
        "\n",
        "metrics = model.val()\n",
        "\n",
        "results = model.predict('/content/Mukesh.jpg', save=True)\n",
        "\n",
        "print(\"Model weights saved at:\", model.ckpt_path)"
      ],
      "metadata": {
        "id": "Q089Arq-cSuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Create Embeddings and Recognise faces**\n",
        "\n",
        "We created a process to generate and save face embeddings with corresponding labels using a trained YOLOv8 model for face detection and the DeepFace library for embedding generation. The YOLOv8 model detects faces in images from a specified folder, and each detected face is cropped using its bounding box. The cropped face is resized and passed to the DeepFace library to generate embeddings using the FaceNet model. Each embedding is associated with a label, extracted from the image filename, and stored in a dictionary. The annotated images are optionally displayed, and the face database, containing all embeddings and labels, is saved as a .pkl file for future use. This setup enables efficient and reusable face recognition."
      ],
      "metadata": {
        "id": "KMFaVTjEdEcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics==8.0.20\n",
        "!pip install deepface"
      ],
      "metadata": {
        "id": "-kq28R--dmrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from deepface import DeepFace\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pickle  # For saving embeddings and labels\n",
        "\n",
        "# Load the YOLO model\n",
        "model = YOLO(\"/content/drive/MyDrive/YoloV8best.pt\")  # Replace with the path to your trained YOLO model\n",
        "\n",
        "# Function to generate embeddings from the cropped face using DeepFace\n",
        "def get_face_embedding(face_image):\n",
        "    embedding = DeepFace.represent(face_image, model_name='Facenet', enforce_detection=False)[0]['embedding']\n",
        "    return embedding\n",
        "\n",
        "# Dictionary to store embeddings and labels\n",
        "face_database = {}\n",
        "\n",
        "# Path to the folder containing images\n",
        "folder_path = '/content/drive/MyDrive/Test_Data/Train'  # Replace with the path to your folder\n",
        "\n",
        "# Loop through all images in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.lower().endswith(('.jpg', '.jpeg', '.png')):  # Filter for image files\n",
        "        image_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Extract the label from the image filename (e.g., \"abc.jpg\" -> \"abc\")\n",
        "        label = os.path.splitext(filename)[0]\n",
        "\n",
        "        # Predict faces using the trained YOLO model\n",
        "        results = model.predict(image_path)\n",
        "\n",
        "        # Loop through each detected face in the results\n",
        "        for result in results[0].boxes.data:  # result[0] contains the detection result\n",
        "            x1, y1, x2, y2 = result[:4]  # Bounding box coordinates (x1, y1, x2, y2)\n",
        "\n",
        "            # Crop the face from the image using the bounding box\n",
        "            face_image = image[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "            # Optionally, resize the face image if needed for the model\n",
        "            face_image = cv2.resize(face_image, (160, 160))  # Example for FaceNet or similar models\n",
        "\n",
        "            # Get the embedding for the cropped face\n",
        "            embedding = get_face_embedding(face_image)\n",
        "\n",
        "            # Add the embedding and label to the face database\n",
        "            face_database[label] = embedding\n",
        "\n",
        "            # Optionally, print or save the embeddings with labels\n",
        "            print(f\"Processed {label}: Embedding added.\")\n",
        "\n",
        "            # Draw bounding box and label on the image\n",
        "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "            cv2.putText(image, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        # Display the processed image\n",
        "        #cv2_imshow(image)\n",
        "\n",
        "# Save the face database for future use\n",
        "with open(\"face_database.pkl\", \"wb\") as file:\n",
        "    pickle.dump(face_database, file)\n",
        "\n",
        "print(\"Face database saved successfully!\")\n"
      ],
      "metadata": {
        "id": "VEyfEa0edpjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To validate and inspect the saved face embeddings, we loaded the previously created pickle file containing the face database. The number of entries (embeddings) in the file was counted and printed, providing an overview of the data stored. Additionally, the labels associated with these embeddings were displayed to verify the correctness and completeness of the saved data. This step ensures that the embedding generation and saving process was successful and that the data is ready for subsequent recognition tasks."
      ],
      "metadata": {
        "id": "OwTnjvmUd52i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load the pickle file\n",
        "with open(\"face_database.pkl\", \"rb\") as file:\n",
        "    face_database = pickle.load(file)\n",
        "\n",
        "# Check the number of entries\n",
        "print(f\"Number of entries in the pickle file: {len(face_database)}\")\n",
        "\n",
        "# Optional: Print the keys (labels) to inspect the data\n",
        "print(\"Labels in the pickle file:\", list(face_database.keys()))\n"
      ],
      "metadata": {
        "id": "dEtGzdSGdu94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we implemented a face recognition system by utilizing embeddings stored in a pre-trained face database. Test images were processed to detect faces using a YOLOv8 model trained specifically for face detection. For each detected face, embeddings were generated using the DeepFace library with the FaceNet model. These embeddings were compared against the stored embeddings in the database using cosine similarity.\n",
        "\n",
        "A label was assigned to each detected face based on the highest similarity score exceeding a defined threshold, ensuring accurate identification. Bounding boxes and labels were drawn on the test images to indicate recognized faces, and the annotated images were saved in an output folder for further analysis. This process enabled us to effectively identify faces in the test dataset."
      ],
      "metadata": {
        "id": "O_Xvb0gZeOaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from deepface import DeepFace\n",
        "import pickle\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the YOLO model\n",
        "model = YOLO(\"/content/drive/MyDrive/YoloV8best.pt\")  # Replace with the path to your trained YOLO model\n",
        "\n",
        "# Load the saved face database\n",
        "with open(\"face_database.pkl\", \"rb\") as file:\n",
        "    face_database = pickle.load(file)\n",
        "\n",
        "# Function to generate embeddings from the cropped face using DeepFace\n",
        "def get_face_embedding(face_image):\n",
        "    embedding = DeepFace.represent(face_image, model_name='Facenet', enforce_detection=False)[0]['embedding']\n",
        "    return embedding\n",
        "\n",
        "# Threshold for cosine similarity (tune based on your use case)\n",
        "SIMILARITY_THRESHOLD = 0.5\n",
        "\n",
        "# Path to the folder containing test images\n",
        "test_folder_path = '/content/drive/MyDrive/Test_Data/Test'  # Replace with your folder path\n",
        "\n",
        "# Loop through all images in the folder\n",
        "for filename in os.listdir(test_folder_path):\n",
        "    if filename.lower().endswith(('.jpg', '.jpeg', '.png')):  # Filter for image files\n",
        "        test_image_path = os.path.join(test_folder_path, filename)\n",
        "\n",
        "        # Read the test image\n",
        "        test_image = cv2.imread(test_image_path)\n",
        "\n",
        "        # Predict faces using the trained YOLO model\n",
        "        results = model.predict(test_image_path)\n",
        "\n",
        "        # Loop through each detected face in the results\n",
        "        for result in results[0].boxes.data:  # result[0] contains the detection result\n",
        "            x1, y1, x2, y2 = result[:4]  # Bounding box coordinates (x1, y1, x2, y2)\n",
        "\n",
        "            # Crop the face from the image using the bounding box\n",
        "            face_image = test_image[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "            # Resize the face image if needed for the model\n",
        "            face_image = cv2.resize(face_image, (160, 160))  # Example for FaceNet or similar models\n",
        "\n",
        "            # Get the embedding for the cropped face\n",
        "            new_embedding = get_face_embedding(face_image)\n",
        "\n",
        "            # Initialize variables to track the best match\n",
        "            best_label = \"Unknown\"\n",
        "            best_similarity = 0\n",
        "\n",
        "            # Compare the new embedding with stored embeddings in the face database\n",
        "            for label, stored_embedding in face_database.items():\n",
        "                similarity = cosine_similarity([new_embedding], [stored_embedding])[0][0]\n",
        "                if similarity > best_similarity and similarity > SIMILARITY_THRESHOLD:\n",
        "                    best_label = label\n",
        "                    best_similarity = similarity\n",
        "\n",
        "            # Draw bounding box and label on the image\n",
        "            cv2.rectangle(test_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "            cv2.putText(test_image, best_label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "            print(f\"Image: {filename}, Detected face is labeled as: {best_label} (similarity: {best_similarity:.2f})\")\n",
        "\n",
        "        # Save or display the image with labels\n",
        "        output_image_path = os.path.join('/content/verified_images', filename)  # Adjust the output folder as needed\n",
        "        os.makedirs(os.path.dirname(output_image_path), exist_ok=True)\n",
        "        cv2.imwrite(output_image_path, test_image)\n",
        "\n",
        "print(\"Face recognition completed for all images in the folder.\")\n"
      ],
      "metadata": {
        "id": "cgmgSAPreBRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Enhancing Face Recognition Model to Handle Face Orientations**\n",
        "\n",
        "# **Objective:**\n",
        "\n",
        "To improve the face recognition model’s ability to recognize faces captured from various angles and orientations, rather than limiting it to frontal face views.\n",
        "\n",
        "# **Current Limitation:**\n",
        "\n",
        "The existing face recognition system effectively detects faces, creates embeddings, and recognizes identities. However, the recognition accuracy drops significantly when a person’s face is oriented away from the frontal view, such as side profiles or tilted angles. This limitation arises due to the model’s training dataset consisting predominantly of frontal face images, which leads to inadequate generalization for non-frontal orientations.\n",
        "\n",
        "# **Proposed Solution:**\n",
        "\n",
        "To address this issue, we propose the following enhancements\n",
        "\n",
        "\n",
        "1.   **Diversify the training DataSet**\n",
        "\n",
        "      * Include images with varied face orientations in the training dataset, such as:\n",
        "           * Left and right side profiles.\n",
        "           *\tTilted faces (upwards and downwards).\n",
        "           * Rotated faces at various angles.\n",
        "           *\tSlightly occluded faces (e.g., partially covered by hair or accessories).\n",
        "      *Gather images of individuals from multiple perspectives to ensure comprehensive coverage of possible real-world scenarios.\n",
        "\n",
        "2.   **Augmentation Techniques**\n",
        "      \n",
        "      * Apply data augmentation techniques to artificially create face images with different orientations, including:\n",
        "           * Rotation at random angles.\n",
        "           * Flipping (horizontal and vertical).\n",
        "           * Adding noise, shadows, or blurring to simulate diverse environmental conditions.\n",
        "\n",
        "3. **Multiview Embedding Creation**\n",
        "\n",
        "     * Capture embeddings for each individual’s face across multiple orientations.\n",
        "     * During recognition, compare the test face’s embedding with all stored embeddings (frontal and non-frontal) for the closest match.\n",
        "\n",
        "\n",
        "# **Outcomes**\n",
        "\n",
        "Implementing these enhancements will significantly improve the face recognition model’s ability to identify individuals accurately, irrespective of their face orientation. This will make the system more robust and suitable for real-world applications like surveillance, attendance systems, and access control.\n",
        "\n",
        "# **Next Steps**\n",
        "   * Collect and curate a diverse dataset with multi-view face images.\n",
        "   * Implement data augmentation and fine-tuning.\n",
        "   * Evaluate the model’s performance with an emphasis on recognizing side profiles and tilted faces.\n",
        "   * Iterate and refine based on testing results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3eOCb_6nedzM"
      }
    }
  ]
}